{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73c7302",
   "metadata": {},
   "source": [
    "# GenAI Governance Layer for Higher Education\n",
    "## Research Framework & Methodology Documentation\n",
    "\n",
    "This notebook documents the complete research framework for building an **executable, verifiable, and transparent governance system for Generative AI in higher education**.\n",
    "\n",
    "### Key Challenge\n",
    "- GenAI adoption in higher education has outpaced policy infrastructure\n",
    "- Current approaches: manual, brittle, untrustworthy\n",
    "- Problems:\n",
    "  - Policies as prose PDFs → inconsistent interpretation\n",
    "  - No decision traceability → disputes unresolvable\n",
    "  - Trust gaps in AI-based guidance → hallucination-prone chatbots\n",
    "  - Opacity in AI use → eroded student trust\n",
    "\n",
    "### Research Questions\n",
    "1. Can policy-as-code enforcement be made practical for educational governance?\n",
    "2. Can RAG systems answer policy questions with verified citations and low hallucination?\n",
    "3. Does privacy-preserving transparency increase student trust and perceived fairness?\n",
    "\n",
    "### Novel Contributions\n",
    "1. **First integrated policy-as-code + verified RAG + transparency system** for educational AI\n",
    "2. **Operationalized RAG verification** for policy domains (citation correctness, entailment, consistency)\n",
    "3. **Privacy-preserving AI-use transparency** for students (metadata-only logging)\n",
    "4. **Production-ready reference implementation** with reproducible evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7e3f",
   "metadata": {},
   "source": [
    "## 1. System Architecture Overview\n",
    "\n",
    "**Key Components**:\n",
    "1. **Policy Compiler**: Form → JSON + conflict detection\n",
    "2. **Decision Engine**: f_policy(policy, context) → (ALLOW/DENY/REQUIRE_JUSTIFICATION, obligations, trace)\n",
    "3. **Transparency Ledger**: Append-only metadata logs + aggregation\n",
    "4. **RAG Copilot**: Verified answer generation (citation correctness >95%, hallucination <5%)\n",
    "5. **Dashboards**: Faculty policy authoring, student logs, instructor analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Analysis environment initialized.\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b64c5",
   "metadata": {},
   "source": [
    "## 2. Evaluation Metrics & Targets\n",
    "\n",
    "### 2.1 Policy Enforcement Accuracy\n",
    "- **Target**: >90%\n",
    "- **Metric**: (# correct decisions) / (total test cases)\n",
    "- **Breakdown**: Per decision type (ALLOW, DENY, REQUIRE_JUSTIFICATION)\n",
    "\n",
    "### 2.2 RAG Factuality Metrics\n",
    "- **Citation Correctness**: >95% of quoted text actually exists in source\n",
    "- **Hallucination Rate**: <5% of claims unsupported by context\n",
    "- **Answer Correctness**: >90% of answers match expert gold labels\n",
    "\n",
    "### 2.3 User Experience Metrics\n",
    "- **Authoring Time**: >50% reduction vs. free-form\n",
    "- **Student Trust (SUS)**: >75 points, +15 point improvement over control\n",
    "- **Perceived Fairness**: >4/5 on Likert scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation targets and baselines\n",
    "metrics_targets = {\n",
    "    'Enforcement Accuracy': {\n",
    "        'target': 0.90,\n",
    "        'baseline': 0.75,  # Manual baseline\n",
    "        'unit': '%',\n",
    "        'description': 'Correct policy decisions'\n",
    "    },\n",
    "    'Conflict Detection F1': {\n",
    "        'target': 0.85,\n",
    "        'baseline': 0.0,\n",
    "        'unit': 'F1 score',\n",
    "        'description': 'Detecting overlapping/contradictory policies'\n",
    "    },\n",
    "    'Citation Correctness': {\n",
    "        'target': 0.95,\n",
    "        'baseline': 0.75,  # Naive RAG\n",
    "        'unit': '%',\n",
    "        'description': 'Correctly attributed quotes in answers'\n",
    "    },\n",
    "    'Answer Correctness': {\n",
    "        'target': 0.90,\n",
    "        'baseline': 0.65,  # Naive RAG\n",
    "        'unit': '%',\n",
    "        'description': 'Correct policy Q&A responses'\n",
    "    },\n",
    "    'Hallucination Rate': {\n",
    "        'target': 0.05,\n",
    "        'baseline': 0.20,  # Naive RAG\n",
    "        'unit': '%',\n",
    "        'description': 'Unsupported claims (lower is better)'\n",
    "    },\n",
    "    'Authoring Time Reduction': {\n",
    "        'target': 0.50,\n",
    "        'baseline': 0.0,\n",
    "        'unit': '%',\n",
    "        'description': 'Time saved with template vs. free-form'\n",
    "    },\n",
    "    'Student Trust (SUS)': {\n",
    "        'target': 75,\n",
    "        'baseline': 60,  # Control group\n",
    "        'unit': 'points',\n",
    "        'description': 'System Usability Scale (treatment vs control)'\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_targets).T\n",
    "metrics_df['improvement'] = metrics_df['target'] - metrics_df['baseline']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION TARGETS & BASELINES\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1cc28",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "\n",
    "### 3.1 Policy Corpus (N=40-60)\n",
    "- Source: Top 50 US universities (MIT, Stanford, Berkeley, Cornell, etc.)\n",
    "- Format: Schemaified JSON with attribution\n",
    "- Output: `datasets/policies_corpus/policies_canonical.json`\n",
    "\n",
    "### 3.2 Expert-Annotated Q&A Benchmark (N=80-100)\n",
    "- Expert annotations: 2 raters, Cohen's kappa >0.70\n",
    "- Gold labels: Yes/No/Maybe + exact citing clauses\n",
    "- Output: `datasets/benchmark_qa.json`\n",
    "\n",
    "### 3.3 Enforcement Scenario Test Suite (N=40-50)\n",
    "- Coverage: Allowed (10-15), Denied (8-10), Overrides (5), Conflicts (5-8), Ambiguous (5)\n",
    "- Output: `datasets/benchmark_scenarios.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset specifications\n",
    "datasets = {\n",
    "    'Policy Corpus': {\n",
    "        'size_estimate': 50,\n",
    "        'effort_hours': 40,\n",
    "        'curation_time_per_policy': 3,\n",
    "        'output_file': 'datasets/policies_corpus/policies_canonical.json'\n",
    "    },\n",
    "    'Q&A Benchmark': {\n",
    "        'size_estimate': 90,\n",
    "        'effort_hours': 50,\n",
    "        'annotation_time_per_question': 0.5,\n",
    "        'raters': 2,\n",
    "        'target_kappa': 0.70,\n",
    "        'output_file': 'datasets/benchmark_qa.json'\n",
    "    },\n",
    "    'Scenario Test Suite': {\n",
    "        'size_estimate': 45,\n",
    "        'effort_hours': 20,\n",
    "        'allowed_scenarios': 12,\n",
    "        'denied_scenarios': 10,\n",
    "        'override_scenarios': 5,\n",
    "        'conflict_scenarios': 8,\n",
    "        'ambiguous_scenarios': 5,\n",
    "        'output_file': 'datasets/benchmark_scenarios.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "datasets_df = pd.DataFrame(datasets).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SPECIFICATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(datasets_df.to_string())\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Curation Effort: {datasets_df['effort_hours'].sum()} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660553a",
   "metadata": {},
   "source": [
    "## 4. Experimental Design\n",
    "\n",
    "### Study 1: Faculty Usability Study (N=12)\n",
    "- **Design**: Within-subject\n",
    "- **Duration**: ~60 minutes per participant\n",
    "- **Measures**: Time, errors, SUS score, qualitative feedback\n",
    "- **Target**: Template mode >50% faster, >70% fewer errors\n",
    "\n",
    "### Study 2: RAG Benchmark Evaluation (Offline)\n",
    "- **Design**: Offline evaluation on Q&A benchmark\n",
    "- **Measures**: Citation correctness, hallucination rate, answer accuracy\n",
    "- **Baselines**: Naive RAG without verification\n",
    "\n",
    "### Study 3: Student Transparency Study (N=50, RCT)\n",
    "- **Design**: Randomized controlled trial\n",
    "- **Conditions**: Treatment (logs visible) vs Control (no logs)\n",
    "- **Measures**: SUS score, perceived fairness, privacy comfort\n",
    "- **Target**: SUS +15 points, fairness >4/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experimental design details\n",
    "studies = {\n",
    "    'Study 1: Faculty Usability': {\n",
    "        'design': 'Within-subject',\n",
    "        'n': 12,\n",
    "        'duration_minutes': 60,\n",
    "        'measures': ['authoring_time', 'error_count', 'SUS_score', 'qualitative'],\n",
    "        'incentive': '$50 gift card',\n",
    "        'timeline': 'Weeks 2-3'\n",
    "    },\n",
    "    'Study 2: RAG Benchmark': {\n",
    "        'design': 'Offline evaluation',\n",
    "        'test_cases': 90,\n",
    "        'duration_minutes': '(automated)',\n",
    "        'measures': ['citation_correctness', 'hallucination_rate', 'answer_accuracy'],\n",
    "        'annotation_effort': '2 experts',\n",
    "        'timeline': 'Weeks 4-5'\n",
    "    },\n",
    "    'Study 3: Student Transparency (RCT)': {\n",
    "        'design': 'Randomized Controlled Trial',\n",
    "        'n_treatment': 25,\n",
    "        'n_control': 25,\n",
    "        'measures': ['SUS_score', 'perceived_fairness', 'privacy_comfort'],\n",
    "        'survey_minutes': 10,\n",
    "        'timeline': 'Full semester (Week 7 intervention)'\n",
    "    }\n",
    "}\n",
    "\n",
    "studies_df = pd.DataFrame(studies).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENTAL DESIGN SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(studies_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00ad0e",
   "metadata": {},
   "source": [
    "## 5. Implementation Timeline\n",
    "\n",
    "### Phase 1: Research Preparation (Weeks 1-2)\n",
    "- Finalize ethics approval (IRB for human studies)\n",
    "- Set up GitHub repo + CI/CD\n",
    "- Curate policy corpus\n",
    "\n",
    "### Phase 2: Core Implementation (Weeks 3-8)\n",
    "- Policy compiler + conflict detector\n",
    "- Governance middleware + decision engine\n",
    "- Transparency ledger + aggregation\n",
    "- RAG component + verification pipeline\n",
    "- Frontend UI\n",
    "\n",
    "### Phase 3: Evaluation (Weeks 9-12)\n",
    "- Faculty usability study\n",
    "- RAG benchmark evaluation\n",
    "- Student transparency study\n",
    "- Data analysis\n",
    "\n",
    "### Phase 4: Publication (Weeks 13-16)\n",
    "- Write paper (methods, results, discussion, ethics)\n",
    "- Create reproducibility package\n",
    "- Submit to peer-reviewed venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timeline visualization\n",
    "timeline_phases = {\n",
    "    'Phase 1: Preparation': {\n",
    "        'weeks': '1-2',\n",
    "        'tasks': ['Ethics approval', 'GitHub setup', 'Policy curation'],\n",
    "        'effort_hours': 30\n",
    "    },\n",
    "    'Phase 2: Implementation': {\n",
    "        'weeks': '3-8',\n",
    "        'tasks': ['Policy compiler', 'Decision engine', 'Transparency ledger', 'RAG copilot', 'Frontend'],\n",
    "        'effort_hours': 200\n",
    "    },\n",
    "    'Phase 3: Evaluation': {\n",
    "        'weeks': '9-12',\n",
    "        'tasks': ['Usability study', 'RAG benchmark', 'Student study', 'Data analysis'],\n",
    "        'effort_hours': 150\n",
    "    },\n",
    "    'Phase 4: Publication': {\n",
    "        'weeks': '13-16',\n",
    "        'tasks': ['Writing', 'Reproducibility package', 'Submission'],\n",
    "        'effort_hours': 80\n",
    "    }\n",
    "}\n",
    "\n",
    "timeline_df = pd.DataFrame(timeline_phases).T\n",
    "timeline_df['total_effort'] = timeline_df['effort_hours'].sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION TIMELINE\")\n",
    "print(\"=\"*80)\n",
    "for phase, data in timeline_phases.items():\n",
    "    print(f\"\\n{phase} (Weeks {data['weeks']}): {data['effort_hours']} hours\")\n",
    "    for task in data['tasks']:\n",
    "        print(f\"  ✓ {task}\")\n",
    "\n",
    "total_hours = sum(p['effort_hours'] for p in timeline_phases.values())\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL ESTIMATED EFFORT: {total_hours} hours (~{total_hours/40} FT weeks)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ff2ad",
   "metadata": {},
   "source": [
    "## 6. Success Criteria\n",
    "\n",
    "### Technical Success\n",
    "- ✓ All components deployed and tested\n",
    "- ✓ Enforcement accuracy >90%\n",
    "- ✓ Citation correctness >95%\n",
    "- ✓ Hallucination rate <5%\n",
    "- ✓ Full test coverage (unit, integration, property-based)\n",
    "\n",
    "### Research Success\n",
    "- ✓ Faculty study: >50% time savings, SUS >75\n",
    "- ✓ Student study: +15 SUS points vs. control\n",
    "- ✓ RAG benchmark: >90% answer correctness\n",
    "- ✓ Publishable results in top-tier venue\n",
    "\n",
    "### Reproducibility Success\n",
    "- ✓ Open-source code (MIT/Apache 2.0)\n",
    "- ✓ Public datasets + benchmarks\n",
    "- ✓ Complete documentation (API, architecture, evaluation)\n",
    "- ✓ CI/CD pipeline for continuous validation\n",
    "- ✓ Reproducible from scratch in <8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3edded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of success criteria\n",
    "success_criteria = {\n",
    "    'Enforcement Accuracy': {'target': '90%', 'verification': 'Scenario test suite'},\n",
    "    'Conflict Detection': {'target': 'F1>0.85', 'verification': 'Policy pair testing'},\n",
    "    'Citation Correctness': {'target': '>95%', 'verification': 'Expert evaluation'},\n",
    "    'Hallucination Rate': {'target': '<5%', 'verification': 'Manual review'},\n",
    "    'Faculty Time Savings': {'target': '>50%', 'verification': 'Usability study'},\n",
    "    'Faculty SUS Score': {'target': '>75', 'verification': 'SUS questionnaire'},\n",
    "    'Student Trust Improvement': {'target': '+15 pts', 'verification': 'RCT study'},\n",
    "    'Test Coverage': {'target': '>80%', 'verification': 'Coverage report'},\n",
    "    'Reproducibility': {'target': '<8hrs', 'verification': 'Fresh build'}\n",
    "}\n",
    "\n",
    "success_df = pd.DataFrame(success_criteria).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUCCESS CRITERIA & VERIFICATION METHODS\")\n",
    "print(\"=\"*80)\n",
    "print(success_df.to_string())\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
